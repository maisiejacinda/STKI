# -*- coding: utf-8 -*-
"""STKI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wgKNFz6-6PWuVckF4D-r-z3dERCx5JkJ
"""

import streamlit as st
import torch
from transformers import BertTokenizer, BertModel
import torch.nn as nn
import re
import os
import gdown
import traceback

def download_model_from_drive(file_id, destination):
    if os.path.exists(destination):
        return
    gdown.download(f"https://drive.google.com/uc?id={file_id}", destination, quiet=False)

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www.\S+", '', text)
    text = re.sub(r"\d+", '', text)
    text = re.sub(r"[^\w\s]", '', text)
    text = re.sub(r"\s+", ' ', text).strip()
    return text

class IndoBERT_CNN_LSTM(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.conv1 = nn.Conv1d(768, 128, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(128, 64, batch_first=True)
        self.fc = nn.Linear(64, 2)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        x = outputs.last_hidden_state
        x = x.permute(0, 2, 1)
        x = self.conv1(x)
        x = x.permute(0, 2, 1)
        _, (h_n, _) = self.lstm(x)
        logits = self.fc(h_n.squeeze(0))
        return logits

st.set_page_config(page_title="Deteksi Ujaran Kebencian TikTok", layout="wide")
st.title("üí¨üî• Deteksi Ujaran Kebencian pada Komentar TikTok")
st.markdown("Masukkan komentar TikTok di bawah ini:")

input_text = st.text_area("üìù Komentar TikTok", height=200)

if st.button("üîç Deteksi"):
    if input_text.strip() == "":
        st.warning("Komentar tidak boleh kosong!")
    else:
        try:
            device = torch.device("cpu")

            # === GANTI DENGAN FILE-ID MODEL KAMU ===
            download_model_from_drive("YOUR_MODEL_FILE_ID_HERE", "model_hatespeech.pt")

            bert_model = BertModel.from_pretrained('indobenchmark/indobert-base-p1').to(device)
            tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

            model = IndoBERT_CNN_LSTM(bert_model)
            model.load_state_dict(torch.load("model_hatespeech.pt", map_location=device))
            model = model.to(device)
            model.eval()

            cleaned = clean_text(input_text)
            st.write("üßπ Teks setelah dibersihkan:")
            st.code(cleaned)

            # Kata kasar langsung dianggap kebencian
            hate_keywords = ["anjing", "babi", "kontol", "goblok", "tolol", "bego", "bangsat"]

            if any(word in cleaned for word in hate_keywords):
                st.error("‚ùå Komentar Mengandung Ujaran Kebencian (kata kasar terdeteksi)")
                st.stop()

            tokens = tokenizer(cleaned, return_tensors='pt', truncation=True, padding='max_length', max_length=128)
            input_ids = tokens['input_ids'].to(device)
            attention_mask = tokens['attention_mask'].to(device)

            with torch.no_grad():
                output = model(input_ids, attention_mask)
                probs = torch.softmax(output, dim=1)
                pred = torch.argmax(probs, dim=1).item()

            confidence_no_hate = probs[0][0].item()
            confidence_hate = probs[0][1].item()

            st.write(f"üìä Confidence Tidak Kebencian: {confidence_no_hate:.2f}")
            st.write(f"üìä Confidence Kebencian: {confidence_hate:.2f}")

            if pred == 0:
                st.success(f"‚úÖ Komentar Tidak Mengandung Kebencian (Confidence {confidence_no_hate:.2f})")
            else:
                st.error(f"‚ùå Komentar Mengandung Ujaran Kebencian (Confidence {confidence_hate:.2f})")

        except Exception as e:
            st.error("‚ùå Terjadi error saat deteksi.")
            st.code(traceback.format_exc())

import pandas as pd
df = pd.read_csv("tiktok_hatespeech_binary.csv")
print(df.head())

pip install transformers torch gdown scikit-learn pandas numpy

import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

df = pd.read_csv("tiktok_hatespeech_binary.csv")
df['text'] = df['text'].astype(str)
df['label'] = df['label'].astype(int)

import re
def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www.\S+", '', text)
    text = re.sub(r"\d+", '', text)
    text = re.sub(r"[^\w\s]", '', text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

df['text'] = df['text'].apply(clean_text)

tokenizer = BertTokenizer.from_pretrained("indobenchmark/indobert-base-p1")

class TiktokDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=128, return_tensors="pt")
        input_ids = tokens['input_ids'].squeeze(0)
        attention_mask = tokens['attention_mask'].squeeze(0)
        label = torch.tensor(self.labels[idx])
        return input_ids, attention_mask, label

train_texts, test_texts, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

train_ds = TiktokDataset(train_texts.tolist(), train_labels.tolist())
test_ds = TiktokDataset(test_texts.tolist(), test_labels.tolist())

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)
test_loader = DataLoader(test_ds, batch_size=16)

# === Model IndoBERT + CNN + LSTM ===
class IndoBERT_CNN_LSTM(nn.Module):
    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.conv = nn.Conv1d(768, 128, kernel_size=3, padding=1)
        self.lstm = nn.LSTM(128, 64, batch_first=True)
        self.fc = nn.Linear(64, 2)

    def forward(self, input_ids, attention_mask):
        with torch.no_grad():  # Freeze BERT for faster training
            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        x = outputs.last_hidden_state
        x = x.permute(0, 2, 1)
        x = self.conv(x)
        x = x.permute(0, 2, 1)
        _, (h_n, _) = self.lstm(x)
        logits = self.fc(h_n.squeeze(0))
        return logits

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
bert_base = BertModel.from_pretrained("indobenchmark/indobert-base-p1").to(device)
model = IndoBERT_CNN_LSTM(bert_base).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(5):
    model.train()
    total_loss = 0
    for input_ids, attention_mask, labels in train_loader:
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}")

model.eval()
preds, true = [], []

with torch.no_grad():
    for input_ids, attention_mask, labels in test_loader:
        input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)
        outputs = model(input_ids, attention_mask)
        preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())
        true.extend(labels.numpy())

print("\n=== Classification Report ===")
print(classification_report(true, preds, target_names=["Tidak Kebencian", "Mengandung Kebencian"]))

print("\n=== Confusion Matrix ===")
print(confusion_matrix(true, preds))

torch.save(model.state_dict(), "model_hatespeech.pt")
print("\n‚úÖ Model saved as model_hatespeech.pt")

train_losses = []

for epoch in range(5):
    model.train()
    total_loss = 0
    for input_ids, attention_mask, labels in train_loader:
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    train_losses.append(avg_loss)
    print(f"Epoch {epoch+1} | Loss: {avg_loss:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(7,4))
plt.plot(train_losses)
plt.title("Learning Curve - Model IndoBERT CNN-LSTM")
plt.xlabel("Epoch")
plt.ylabel("Training Loss")
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import seaborn as sns
import matplotlib.pyplot as plt

print("\n=== Classification Report ===")
print(classification_report(true, preds, target_names=["Tidak Kebencian", "Mengandung Kebencian"]))

acc = accuracy_score(true, preds)
precision, recall, f1, _ = precision_recall_fscore_support(true, preds, average='binary')

print("\n=== Skor Evaluasi ===")
print(f"Akurasi  : {acc:.4f}")
print(f"Presisi  : {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1 Score : {f1:.4f}")

# Tabel ringkas (langsung bisa copy ke laporan)
print("\nTabel Laporan:")
print(pd.DataFrame({
    "Metrik": ["Akurasi", "Presisi", "Recall", "F1-Score"],
    "Nilai": [acc, precision, recall, f1]
}))

# === Confusion Matrix Visual ===
cm = confusion_matrix(true, preds)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Tidak Kebencian", "Kebencian"],
            yticklabels=["Tidak Kebencian", "Kebencian"])
plt.title("Confusion Matrix")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

def predict_text(text):
    text = clean_text(text)
    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)

    input_ids = encoded['input_ids'].to(device)
    attention_mask = encoded['attention_mask'].to(device)

    with torch.no_grad():
        output = model(input_ids, attention_mask)
        pred = torch.argmax(output, dim=1).item()

    return "hoax" if pred == 1 else "valid"

torch.save(model.state_dict(), "model_hoax_cnn_lstm.pt")